{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bank Classification\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#####   Loading Libraries\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import mean, col, split, col, regexp_extract, when, lit\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import StringIndexer, IndexToString, VectorAssembler, VectorIndexer\n",
        "from pyspark.ml.feature import QuantileDiscretizer, OneHotEncoderEstimator, OneHotEncoder, StringIndexer, VectorIndexer\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
        "\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "spark = SparkSession.builder.appName(\"Spark ML applied on Bank Marketing dataset\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### get data\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Primary storage info\n",
        "account_name = 'synapse11datalake' # primary ADLS account name https://synapse11datalake.dfs.core.windows.net\n",
        "container_name = 'root' # Primary ADLS Gen2 file system from Synapse Home Page\n",
        "relative_path = 'Raw' #  relative folder path\n",
        "filename =  'bank.csv'\n",
        "bank_data_path = 'abfss://%s@%s.dfs.core.windows.net/%s/%s' % (container_name, account_name, relative_path,filename)\n",
        "\n",
        "bank_df = spark.read.csv(bank_data_path, header = 'True', inferSchema = 'True')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "bank_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "bank_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "groupBy_clients = bank_df.groupBy(\"deposit\").count()\n",
        "\n",
        "groupBy_clients.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "collapsed": false,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "deposit"
            ],
            "values": [
              "count"
            ],
            "yLabel": "count",
            "xLabel": "deposit",
            "aggregation": "SUM",
            "aggByBackend": true,
            "isValid": true,
            "inValidMsg": null,
            "stacked": true
          },
          "aggData": "{\"count\":{\"no\":5873,\"yes\":5289}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "display(groupBy_clients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "bank_df.describe([t[0] for t in bank_df.dtypes if t[1] == 'int']).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "collapsed": false,
        "diagram": {
          "activateDiagramType": 2,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "job"
            ],
            "values": [
              "count"
            ],
            "yLabel": "count",
            "xLabel": "job",
            "aggregation": "SUM",
            "aggByBackend": false,
            "isValid": true,
            "inValidMsg": null
          },
          "aggData": "{\"count\":{\"admin.\":1334,\"blue-collar\":1944,\"entrepreneur\":328,\"housemaid\":274,\"management\":2566,\"retired\":778,\"self-employed\":405,\"services\":923,\"student\":360,\"technician\":1823,\"unemployed\":357,\"unknown\":70}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "display(bank_df.groupBy(\"job\").count())\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Preprocess Data\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "def get_dummy(df, categoricalCols, continuousCols, labelCol):\n",
        "  \n",
        "  indexers = [StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categoricalCols]\n",
        "\n",
        "  encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
        "                             outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
        "              for indexer in indexers]\n",
        "\n",
        "  assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
        "                              + continuousCols, outputCol=\"features\")\n",
        "  \n",
        "  indexer = StringIndexer(inputCol=labelCol, outputCol='indexedLabel')\n",
        "\n",
        "  pipeline = Pipeline(stages = indexers + encoders + [assembler] + [indexer])\n",
        "\n",
        "  model=pipeline.fit(df)\n",
        "  data = model.transform(df)\n",
        " \n",
        "  data = data.withColumn('label', col(labelCol))\n",
        "  \n",
        "  return data.select('features', 'indexedLabel', 'label'), StringIndexer(inputCol='label').fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Transform data\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
        "numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
        "(bank_df, labelindexer) = get_dummy(bank_df, categoricalColumns, numericCols, 'deposit')\n",
        "bank_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Identify categorical features and index them\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(bank_df)\n",
        "\n",
        "featureIndexer.transform(bank_df).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "bank_df.show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Split and Training datasets\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "(trainingData, testData) = bank_df.randomSplit([0.8, 0.2], seed=10)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "print(\"The first 5 samples of the Training Dataset:\")\n",
        "trainingData.show(5, False)\n",
        "print(\"The first 5 samples of the Test Dataset:\")\n",
        "testData.show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate Models\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "#lr = LogisticRegression(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\") # using this line if you would using indexedFeatures instead features column\n",
        "lr = LogisticRegression(labelCol=\"indexedLabel\", featuresCol=\"features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Pipeline\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelindexer.labels) \n",
        "\n",
        "pipeline = Pipeline(stages=[featureIndexer, lr, labelConverter])\n",
        "\n",
        "lrModel = pipeline.fit(trainingData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {},
      "source": [
        "predictions = lrModel.transform(testData)\n",
        "\n",
        "predictions.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {},
      "source": [
        "predictions.select(\"features\", \"label\", \"probability\", \"predictedLabel\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {},
      "source": [
        "cm = predictions.select(\"label\", \"predictedLabel\")          \n",
        "cm.groupby('label').agg({'label': 'count'}).show()  \n",
        "cm.groupby('predictedLabel').agg({'predictedLabel': 'count'}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {},
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "print(\"The Accuracy for test set is {}\".format(evaluator.evaluate(predictions)))\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create a Confusion Matrix\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "predictionAndLabel = predictions.select(\"prediction\", \"indexedLabel\").rdd\n",
        "\n",
        "# Instantiate metrics object \n",
        "metricsMulti = MulticlassMetrics(predictionAndLabel)\n",
        "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\n",
        "# Overall statistics \n",
        "confusionMatrix = metricsMulti.confusionMatrix()\n",
        "precision = metricsMulti.precision(label=1) \n",
        "recall = metricsMulti.recall(label=1) \n",
        "f1Score = metricsMulti.fMeasure() \n",
        "print(\"Summary Stats\")\n",
        "print(\"Confusion Matrix = \\n %s\" % confusionMatrix)\n",
        "print(\"Precision = %s\" % precision) \n",
        "print(\"Recall = %s\" % recall) \n",
        "print(\"F1 Score = %s\" % f1Score) \n",
        "\n",
        "# Area under precision-recall curve \n",
        "print(\"Area under PR = %s\" % metricsBinary.areaUnderPR) \n",
        "# Area under ROC curve \n",
        "print(\"Area under ROC = %s\" % metricsBinary.areaUnderROC)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model tuning\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "print(lr.explainParams())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameter Tuning\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {},
      "source": [
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "             .addGrid(lr.maxIter, [1, 5, 10])\n",
        "             .build())\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create and run 5-fold CrossValidator\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {},
      "source": [
        "#cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
        "#pipeline = Pipeline(stages=[featureIndexer, cv, labelConverter])\n",
        "#cvModel = pipeline.fit(trainingData)\n",
        "\n",
        "pipeline = Pipeline(stages=[featureIndexer, lr, labelConverter]) \n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5, parallelism=10, seed=100)\n",
        "cvModel = cv.fit(trainingData)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Use the new data for testing\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {},
      "source": [
        "predictions = cvModel.transform(testData)\n",
        "\n",
        "predictions.select(\"features\", \"label\", \"probability\", \"predictedLabel\").show(5)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate the best model\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {},
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "print(\"The Accuracy for test set is {}\".format(evaluator.evaluate(predictions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {},
      "source": [
        "predictionAndLabel = predictions.select(\"prediction\", \"indexedLabel\").rdd\n",
        "\n",
        "# Instantiate metrics object \n",
        "metricsMulti = MulticlassMetrics(predictionAndLabel)\n",
        "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\n",
        "# Overall statistics \n",
        "confusionMatrix = metricsMulti.confusionMatrix()\n",
        "precision = metricsMulti.precision(label=1) \n",
        "recall = metricsMulti.recall(label=1) \n",
        "f1Score = metricsMulti.fMeasure() \n",
        "print(\"Summary Stats\")\n",
        "print(\"Confusion Matrix = \\n %s\" % confusionMatrix)\n",
        "print(\"Precision = %s\" % precision) \n",
        "print(\"Recall = %s\" % recall) \n",
        "print(\"F1 Score = %s\" % f1Score) \n",
        "\n",
        "# Area under precision-recall curve \n",
        "print(\"Area under PR = %s\" % metricsBinary.areaUnderPR) \n",
        "# Area under ROC curve \n",
        "print(\"Area under ROC = %s\" % metricsBinary.areaUnderROC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\n",
        "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Evaluate Decision Tree Algorithms\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create initial Decision Tree Model\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")\n",
        "\n",
        "# Train model with Training Data.\n",
        "dtModel = dt.fit(trainingData)\n",
        "\n",
        "# Make predictions on test data.\n",
        "predictions = dtModel.transform(testData)\n",
        "\n",
        "# Evaluate the model by computing the metrics. \n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "print(\"The Accuracy for test set is {}\".format(evaluator.evaluate(predictions)))\n",
        "\n",
        "print(\"===============================================\")\n",
        "\n",
        "predictionAndLabel = predictions.select(\"prediction\", \"indexedLabel\").rdd\n",
        "\n",
        "# Instantiate metrics object \n",
        "metricsMulti = MulticlassMetrics(predictionAndLabel)\n",
        "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\n",
        "# Overall statistics \n",
        "confusionMatrix = metricsMulti.confusionMatrix()\n",
        "precision = metricsMulti.precision(label=1) \n",
        "recall = metricsMulti.recall(label=1) \n",
        "f1Score = metricsMulti.fMeasure() \n",
        "print(\"Summary Stats\")\n",
        "print(\"Confusion Matrix = \\n %s\" % confusionMatrix)\n",
        "print(\"Precision = %s\" % precision) \n",
        "print(\"Recall = %s\" % recall) \n",
        "print(\"F1 Score = %s\" % f1Score) \n",
        "\n",
        "# Area under precision-recall curve \n",
        "print(\"Area under PR = %s\" % metricsBinary.areaUnderPR) \n",
        "# Area under ROC curve \n",
        "print(\"Area under ROC = %s\" % metricsBinary.areaUnderROC)\n",
        "\n",
        "print(\"===============================================\")\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\n",
        "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predictions)))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hyper parameter tuning\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {},
      "source": [
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(dt.maxDepth, [1, 2, 6, 10])\n",
        "             .addGrid(dt.maxBins, [20, 40, 80])\n",
        "             .build())\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\n",
        "\n",
        "pipeline = Pipeline(stages=[featureIndexer, dt, labelConverter]) \n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5, parallelism=10, seed=100)\n",
        "cvModel = cv.fit(trainingData)\n",
        "\n",
        "predictions = cvModel.transform(testData)\n",
        "\n",
        "# Evaluate the best model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "print(\"The Accuracy for test set is {}\".format(evaluator.evaluate(predictions)))\n",
        "\n",
        "print(\"===============================================\")\n",
        "\n",
        "predictionAndLabel = predictions.select(\"prediction\", \"indexedLabel\").rdd\n",
        "\n",
        "# Instantiate metrics object \n",
        "metricsMulti = MulticlassMetrics(predictionAndLabel)\n",
        "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\n",
        "# Overall statistics \n",
        "confusionMatrix = metricsMulti.confusionMatrix()\n",
        "precision = metricsMulti.precision(label=1) \n",
        "recall = metricsMulti.recall(label=1) \n",
        "f1Score = metricsMulti.fMeasure() \n",
        "print(\"Summary Stats\")\n",
        "print(\"Confusion Matrix = \\n %s\" % confusionMatrix)\n",
        "print(\"Precision = %s\" % precision) \n",
        "print(\"Recall = %s\" % recall) \n",
        "print(\"F1 Score = %s\" % f1Score) \n",
        "\n",
        "# Area under precision-recall curve \n",
        "print(\"Area under PR = %s\" % metricsBinary.areaUnderPR) \n",
        "# Area under ROC curve \n",
        "print(\"Area under ROC = %s\" % metricsBinary.areaUnderROC)\n",
        "\n",
        "print(\"===============================================\")\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\n",
        "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predictions)))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create initial Random Forest Classifier\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")\n",
        "\n",
        "# Train model with Training Data.\n",
        "rfModel = rf.fit(trainingData)\n",
        "\n",
        "# Make predictions on test data.\n",
        "predictions = rfModel.transform(testData)\n",
        "\n",
        "# Evaluate the model by computing the metrics. \n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "print(\"The Accuracy for test set is {}\".format(evaluator.evaluate(predictions)))\n",
        "\n",
        "print(\"===============================================\")\n",
        "\n",
        "predictionAndLabel = predictions.select(\"prediction\", \"indexedLabel\").rdd\n",
        "\n",
        "# Instantiate metrics object \n",
        "metricsMulti = MulticlassMetrics(predictionAndLabel)\n",
        "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\n",
        "# Overall statistics \n",
        "confusionMatrix = metricsMulti.confusionMatrix()\n",
        "precision = metricsMulti.precision(label=1) \n",
        "recall = metricsMulti.recall(label=1) \n",
        "f1Score = metricsMulti.fMeasure() \n",
        "print(\"Summary Stats\")\n",
        "print(\"Confusion Matrix = \\n %s\" % confusionMatrix)\n",
        "print(\"Precision = %s\" % precision) \n",
        "print(\"Recall = %s\" % recall) \n",
        "print(\"F1 Score = %s\" % f1Score) \n",
        "\n",
        "# Area under precision-recall curve \n",
        "print(\"Area under PR = %s\" % metricsBinary.areaUnderPR) \n",
        "# Area under ROC curve \n",
        "print(\"Area under ROC = %s\" % metricsBinary.areaUnderROC)\n",
        "\n",
        "print(\"===============================================\")\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\n",
        "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameter Tuning\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {},
      "source": [
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
        "             .addGrid(rf.maxBins, [20, 60])\n",
        "             .addGrid(rf.numTrees, [5, 20])\n",
        "             .build())\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\n",
        "\n",
        "pipeline = Pipeline(stages=[featureIndexer, rf, labelConverter]) \n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5, parallelism=10, seed=100)\n",
        "cvModel = cv.fit(trainingData)\n",
        "\n",
        "predictions = cvModel.transform(testData)\n",
        "\n",
        "# Evaluate the best model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "print(\"The Accuracy for test set is {}\".format(evaluator.evaluate(predictions)))\n",
        "\n",
        "print(\"===============================================\")\n",
        "\n",
        "predictionAndLabel = predictions.select(\"prediction\", \"indexedLabel\").rdd\n",
        "\n",
        "# Instantiate metrics object \n",
        "metricsMulti = MulticlassMetrics(predictionAndLabel)\n",
        "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\n",
        "# Overall statistics \n",
        "confusionMatrix = metricsMulti.confusionMatrix()\n",
        "precision = metricsMulti.precision(label=1) \n",
        "recall = metricsMulti.recall(label=1) \n",
        "f1Score = metricsMulti.fMeasure() \n",
        "print(\"Summary Stats\")\n",
        "print(\"Confusion Matrix = \\n %s\" % confusionMatrix)\n",
        "print(\"Precision = %s\" % precision) \n",
        "print(\"Recall = %s\" % recall) \n",
        "print(\"F1 Score = %s\" % f1Score) \n",
        "\n",
        "# Area under precision-recall curve \n",
        "print(\"Area under PR = %s\" % metricsBinary.areaUnderPR) \n",
        "# Area under ROC curve \n",
        "print(\"Area under ROC = %s\" % metricsBinary.areaUnderROC)\n",
        "\n",
        "print(\"===============================================\")\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\n",
        "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predictions)))"
      ]
    }
  ]
}